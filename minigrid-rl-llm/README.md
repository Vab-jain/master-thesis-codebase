# BabyAI RL-LLM Research Codebase

This codebase implements a comprehensive research pipeline for studying the integration of Large Language Models (LLMs) with Reinforcement Learning (RL) agents in BabyAI environments. The system supports ground truth data collection, LLM optimization, evaluation, RL agent training, and comparison analysis.

## ğŸ—ï¸ Codebase Structure

The codebase is organized into five main modules:

```
minigrid-rl-llm/
â”œâ”€â”€ 1_GT_collection/              # Ground Truth Data Collection
â”‚   â”œâ”€â”€ GT_dataset/               # Collected demonstration data
â”‚   â””â”€â”€ gt_data_collection.py     # Main GT collection script
â”œâ”€â”€ 2_LLM_optimization/           # LLM Optimization & Fine-tuning
â”‚   â”œâ”€â”€ saved_llm_models/         # Trained DSPy models
â”‚   â”œâ”€â”€ ablation_study.py         # Ablation study implementation
â”‚   â”œâ”€â”€ fine_tune_models.py       # Model fine-tuning
â”‚   â”œâ”€â”€ analyze_ablation_results.py # Results analysis
â”‚   â””â”€â”€ main_llm_optimization.py  # Main optimization script
â”œâ”€â”€ 3_LLM_evaluation/             # LLM Model Evaluation
â”‚   â”œâ”€â”€ LLM_evaluation/           # Evaluation results
â”‚   â”œâ”€â”€ llm_evaluation.py         # Evaluation implementation
â”‚   â””â”€â”€ main_llm_evaluation.py    # Main evaluation script
â”œâ”€â”€ 4_RL_agent_training/          # RL Agent Training
â”‚   â”œâ”€â”€ RL_Trained_Agents/        # Trained RL agents
â”‚   â”œâ”€â”€ train_stable_baselines.py # Stable Baselines training
â”‚   â”œâ”€â”€ train_with_hints.py       # Hint-based training
â”‚   â””â”€â”€ main_rl_training.py       # Main training script
â”œâ”€â”€ 5_RL_agent_comparison/        # RL Agent Comparison
â”‚   â”œâ”€â”€ RL_Agents_Comparison/     # Comparison results
â”‚   â””â”€â”€ main_agent_comparison.py  # Main comparison script
â”œâ”€â”€ utils/                        # Shared utilities
â”‚   â”œâ”€â”€ observation_encoder.py    # Text encoding utilities
â”‚   â””â”€â”€ hint_wrapper.py           # Hint wrapper implementation
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ gt_collection_config.yaml
â”‚   â”œâ”€â”€ llm_optimization_config.yaml
â”‚   â”œâ”€â”€ llm_evaluation_config.yaml
â”‚   â”œâ”€â”€ rl_training_config.yaml
â”‚   â””â”€â”€ agent_comparison_config.yaml
â””â”€â”€ sand_*.py                     # Sandboxed test files
```

## ğŸš€ Quick Start

### 1. Ground Truth Data Collection

Collect BabyAI bot demonstrations with text encodings:

```bash
cd 1_GT_collection
python gt_data_collection.py --env-ids BabyAI-GoToObj-v0 BabyAI-GoToLocal-v0 --seeds 42 123
```

### 2. LLM Optimization

Run ablation studies and fine-tuning:

```bash
cd 2_LLM_optimization
python main_llm_optimization.py --mode all
```

### 3. LLM Evaluation

Evaluate trained DSPy models:

```bash
cd 3_LLM_evaluation
python main_llm_evaluation.py --model-path ../2_LLM_optimization/saved_llm_models/subgoal_predictor_ascii.json
```

### 4. RL Agent Training

Train RL agents with or without hints:

```bash
cd 4_RL_agent_training
python main_rl_training.py --env-id BabyAI-GoToObj-v0 --use-hints --hint-type subgoal
```

### 5. Agent Comparison

Compare trained agents:

```bash
cd 5_RL_agent_comparison
python main_agent_comparison.py --pattern "*_no_hints_*" "*_subgoal_freq5_*"
```

## ğŸ“‹ Detailed Usage

### Ground Truth Collection

The GT collection module collects demonstrations from the BabyAI bot and adds multiple text encodings:

- **Natural Language**: Human-readable descriptions
- **ASCII Grid**: Visual grid representation
- **Tuple Lists**: Structured object lists
- **Relative Descriptions**: Position-relative descriptions

```bash
# Using configuration file
python gt_data_collection.py --config ../configs/gt_collection_config.yaml

# Using command line arguments
python gt_data_collection.py \
  --env-ids BabyAI-GoToObj-v0 BabyAI-GoToLocal-v0 \
  --seeds 42 123 456 \
  --output-dir GT_dataset
```

### LLM Optimization

The LLM optimization module supports:

- **Ablation Studies**: Compare different encoding types and bootstrapped examples
- **Fine-tuning**: Train DSPy models for subgoal and action prediction
- **Analysis**: Generate comparison plots and statistics

```bash
# Run complete pipeline
python main_llm_optimization.py --mode all

# Run specific components
python main_llm_optimization.py --mode ablation
python main_llm_optimization.py --mode fine_tuning
python main_llm_optimization.py --mode analysis
```

### LLM Evaluation

Evaluate trained DSPy models on test datasets:

```bash
# Evaluate single model
python main_llm_evaluation.py \
  --model-path ../2_LLM_optimization/saved_llm_models/subgoal_predictor_ascii.json \
  --env-id BabyAI-GoToObj-v0 \
  --encoding-type ascii \
  --hint-type subgoal

# Run multiple evaluations from config
python main_llm_evaluation.py --config ../configs/llm_evaluation_config.yaml
```

### RL Agent Training

Train RL agents using Stable Baselines3 with optional hints:

```bash
# Train baseline agent (no hints)
python main_rl_training.py \
  --env-id BabyAI-GoToObj-v0 \
  --obs-type multi \
  --total-timesteps 50000

# Train agent with BabyAI bot hints
python main_rl_training.py \
  --env-id BabyAI-GoToObj-v0 \
  --obs-type multi \
  --use-hints \
  --hint-type subgoal \
  --hint-frequency 5 \
  --hint-source babyai_bot \
  --total-timesteps 50000

# Train agent with DSPy hints
python main_rl_training.py \
  --env-id BabyAI-GoToObj-v0 \
  --obs-type multi \
  --use-hints \
  --hint-type subgoal \
  --hint-frequency 5 \
  --hint-source dspy \
  --hint-model-path ../2_LLM_optimization/saved_llm_models/subgoal_predictor_ascii.json \
  --total-timesteps 50000

# Run multiple experiments from config
python main_rl_training.py --config ../configs/rl_training_config.yaml
```

### Agent Comparison

Compare trained agents and generate analysis plots:

```bash
# Compare specific agents
python main_agent_comparison.py \
  --experiment-dirs \
    ../4_RL_agent_training/RL_Trained_Agents/BabyAI_GoToObj_v0_multi_no_hints_50000ts_seed42 \
    ../4_RL_agent_training/RL_Trained_Agents/BabyAI_GoToObj_v0_multi_subgoal_freq5_babyai_bot_50000ts_seed42

# Compare using patterns
python main_agent_comparison.py \
  --pattern "*_no_hints_*" "*_subgoal_freq5_*" \
  --plot-title "Baseline vs Subgoal Hints"

# Run predefined comparisons from config
python main_agent_comparison.py --config ../configs/agent_comparison_config.yaml
```

## ğŸ”§ Configuration Files

Each module has a corresponding YAML configuration file in the `configs/` directory:

- `gt_collection_config.yaml`: GT data collection settings
- `llm_optimization_config.yaml`: LLM optimization parameters
- `llm_evaluation_config.yaml`: LLM evaluation settings
- `rl_training_config.yaml`: RL training experiments
- `agent_comparison_config.yaml`: Agent comparison scenarios

## ğŸ¯ Hint System

The hint system provides two types of hints to RL agents:

### Hint Types
- **Subgoal Hints**: High-level planning hints (e.g., "go to the red key")
- **Action Hints**: Low-level control hints (e.g., "turn left", "move forward")

### Hint Sources
- **BabyAI Bot**: Oracle hints from the BabyAI bot
- **DSPy Models**: Learned hints from fine-tuned DSPy models

### Hint Configuration
- **Frequency**: How often hints are provided (every k steps)
- **Probability**: Probability of providing a hint when due
- **Encoding**: Text encoding type for DSPy hints

## ğŸ“Š Output and Results

### GT Collection
- JSON files with demonstrations and text encodings
- Organized by environment and seed

### LLM Optimization
- Trained DSPy models in JSON format
- Ablation study results with plots
- Performance comparison tables

### LLM Evaluation
- Accuracy metrics for each model
- Detailed evaluation reports
- Cross-environment performance analysis

### RL Training
- Trained PPO models with training logs
- Evaluation results and best models
- Training configuration and metadata

### Agent Comparison
- Sample efficiency plots (raw and smoothed)
- Training time comparisons
- Final performance analysis
- Statistical significance tests

## ğŸ› ï¸ Dependencies

```bash
pip install gymnasium minigrid dspy-ai stable-baselines3 torch matplotlib pandas numpy pyyaml
```

## ğŸ“ Notes

- All scripts support both configuration files and command-line arguments
- Results are automatically saved with timestamps
- The hint wrapper supports both vectorized and non-vectorized environments
- Sample efficiency plots include both raw and smoothed curves
- All experiments include proper logging and error handling

## ğŸ”„ Pipeline Workflow

1. **Collect GT Data**: Gather demonstrations from BabyAI bot
2. **Optimize LLMs**: Run ablation studies and fine-tune DSPy models
3. **Evaluate LLMs**: Test model performance on held-out data
4. **Train RL Agents**: Train agents with and without hints
5. **Compare Agents**: Analyze sample efficiency and final performance

This modular design allows for independent experimentation and easy extension of the research pipeline.
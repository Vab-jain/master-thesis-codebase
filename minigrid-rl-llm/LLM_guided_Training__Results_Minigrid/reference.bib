% LLM as Goal Design

@article{prakash_using_2024,
	title = {Using {LLMs} for {Augmenting} {Hierarchical} {Agents} with {Common} {Sense} {Priors}},
	volume = {37},
	copyright = {Copyright (c) 2024 Bharat Prakash, Tim Oates, Tinoosh Mohsenin},
	issn = {2334-0762},
	url = {https://journals.flvc.org/FLAIRS/article/view/135602},
	doi = {10.32473/flairs.37.1.135602},
	language = {en},
	urldate = {2024-10-17},
	journal = {The International FLAIRS Conference Proceedings},
	author = {Prakash, Bharat and Oates, Tim and Mohsenin, Tinoosh},
	month = may,
	year = {2024},
}

@misc{singh_lgr2_2024,
	title = {{LGR2}: {Language} {Guided} {Reward} {Relabeling} for {Accelerating} {Hierarchical} {Reinforcement} {Learning}},
	shorttitle = {{LGR2}},
	url = {http://arxiv.org/abs/2406.05881},
	doi = {10.48550/arXiv.2406.05881},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Singh, Utsav and Bhattacharyya, Pramit and Namboodiri, Vinay P.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05881},
}

@inproceedings{du_guiding_2023,
	title = {Guiding {Pretraining} in {Reinforcement} {Learning} with {Large} {Language} {Models}},
	url = {https://proceedings.mlr.press/v202/du23f.html},
	language = {en},
	urldate = {2024-11-05},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, Cédric and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {8657--8677},
	file = {Full Text PDF:/Users/vj/Zotero/storage/YD9UR8WW/Du et al. - 2023 - Guiding Pretraining in Reinforcement Learning with Large Language Models.pdf:application/pdf},
}

@misc{liu_odyssey_2024,
	title = {Odyssey: {Empowering} {Minecraft} {Agents} with {Open}-{World} {Skills}},
	shorttitle = {Odyssey},
	url = {http://arxiv.org/abs/2407.15325},
	doi = {10.48550/arXiv.2407.15325},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Liu, Shunyu and Li, Yaoru and Zhang, Kongcheng and Cui, Zhenyu and Fang, Wenkai and Zheng, Yuxuan and Zheng, Tongya and Song, Mingli},
	month = oct,
	year = {2024},
	note = {arXiv:2407.15325},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/vj/Zotero/storage/X82VDTLH/Liu et al. - 2024 - Odyssey Empowering Minecraft Agents with Open-World Skills.pdf:application/pdf;Snapshot:/Users/vj/Zotero/storage/U2XMQTUY/2407.html:text/html},
}




% Reward Design

@misc{kwon_reward_2023,
	title = {Reward {Design} with {Language} {Models}},
	url = {http://arxiv.org/abs/2303.00001},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
	month = feb,
	year = {2023},
	note = {arXiv:2303.00001},
	file = {PDF:/Users/vj/Zotero/storage/DASTBR9D/Kwon et al. - 2023 - Reward Design with Language Models.pdf:application/pdf},
}

@misc{wu_read_2024,
	title = {Read and {Reap} the {Rewards}: {Learning} to {Play} {Atari} with the {Help} of {Instruction} {Manuals}},
	shorttitle = {Read and {Reap} the {Rewards}},
	url = {http://arxiv.org/abs/2302.04449},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Wu, Yue and Fan, Yewen and Liang, Paul Pu and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom M.},
	month = jul,
	year = {2024},
	note = {arXiv:2302.04449},
}

@misc{chu_accelerating_2023,
	title = {Accelerating {Reinforcement} {Learning} of {Robotic} {Manipulations} via {Feedback} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.02379},
	doi = {10.48550/arXiv.2311.02379},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Chu, Kun and Zhao, Xufeng and Weber, Cornelius and Li, Mengdi and Wermter, Stefan},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02379},
}

@misc{noauthor_eureka_nodate,
	title = {Eureka {\textbar} {Human}-{Level} {Reward} {Design} via {Coding} {Large} {Language} {Models}},
	url = {https://eureka-research.github.io/},
	urldate = {2024-10-17},
}

@inproceedings{li_auto_2024,
	title = {Auto {MC}-{Reward}: {Automated} {Dense} {Reward} {Design} with {Large} {Language} {Models} for {Minecraft}},
	shorttitle = {Auto {MC}-{Reward}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Li_Auto_MC-Reward_Automated_Dense_Reward_Design_with_Large_Language_Models_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-11-18},
	author = {Li, Hao and Yang, Xue and Wang, Zhaokai and Zhu, Xizhou and Zhou, Jie and Qiao, Yu and Wang, Xiaogang and Li, Hongsheng and Lu, Lewei and Dai, Jifeng},
	year = {2024},
	pages = {16426--16435},
	file = {Full Text PDF:/Users/vj/Zotero/storage/LRSUDJAL/Li et al. - 2024 - Auto MC-Reward Automated Dense Reward Design with Large Language Models for Minecraft.pdf:application/pdf},
}



% LLM as Info Processors

@misc{paischer_history_2023,
	title = {History {Compression} via {Language} {Models} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2205.12258},
	doi = {10.48550/arXiv.2205.12258},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Paischer, Fabian and Adler, Thomas and Patil, Vihang and Bitto-Nemling, Angela and Holzleitner, Markus and Lehner, Sebastian and Eghbal-zadeh, Hamid and Hochreiter, Sepp},
	month = feb,
	year = {2023},
	note = {arXiv:2205.12258},
	keywords = {Minigrid, ICML},
}



% LLM as Policy
@inproceedings{carta2023grounding,
  title={Grounding large language models in interactive environments with online reinforcement learning},
  author={Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle={International Conference on Machine Learning},
  pages={3676--3713},
  year={2023},
  organization={PMLR}
}

@article{li2022pre,
  title={Pre-trained language models for interactive decision-making},
  author={Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky{\"u}rek, Ekin and Anandkumar, Anima and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31199--31212},
  year={2022}
}



% LLM as World Models


@misc{noauthor_dynalang_nodate,
	title = {Dynalang: {Learning} to {Model} the {World} with {Language}},
	shorttitle = {Dynalang},
	url = {https://dynalang.github.io/},
	abstract = {Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model.},
	urldate = {2024-10-17},
}

@misc{micheli_transformers_2023,
	title = {Transformers are {Sample}-{Efficient} {World} {Models}},
	url = {http://arxiv.org/abs/2209.00588},
	doi = {10.48550/arXiv.2209.00588},
	abstract = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Micheli, Vincent and Alonso, Eloi and Fleuret, François},
	month = mar,
	year = {2023},
	note = {arXiv:2209.00588},
}

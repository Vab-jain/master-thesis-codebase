% Deal-or-No-Deal: Environment, Prompting, and RL Training Summary
\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue}

\title{Deal-or-No-Deal: Environment, Prompting, and RL Training Summary}
\author{}
\date{\today}

\begin{document}
\maketitle

\section{Environment}
\textbf{Env ID}: DealOrNoDialog-v0\newline
\textbf{Implementation}: \texttt{deal\_or\_no\_deal\_env/env.py} (\texttt{NegotiationEnv}).

\subsection{Game Description}
This environment simulates a two-agent, multi-issue bargaining problem based on Lewis et al. (2017). The two agents (the learning agent and a partner) must negotiate the division of a fixed pool of items, which are available in three distinct types (e.g., books, hats, balls) with given counts. 

Each agent has its own private \textbf{utility function} (a vector of values, one for each item type), which determines their preference and potential score. The agents negotiate over a limited number of turns (\texttt{max\_turns}) by exchanging dialogue acts (PROPOSE, INSIST, AGREE, DISAGREE, END).

A successful negotiation---a ``deal''---is reached if both agents AGREE on an allocation ($o_A, o_B$) that perfectly conserves the total item pool $i$ (i.e., $o_A + o_B = i$). In this case, the agent receives a reward equal to the dot product of its utility vector and the items it receives: $r_A = u_A \cdot o_A$. The partner receives its own score $r_B = u_B \cdot o_B$. The episode terminates (often with zero reward) if an agent decides to END the negotiation, if a DISAGREE occurs, or if the turn limit is reached. The agent's goal is to maximize its own utility $r_A$ while ensuring the partner also agrees to the deal.

\subsection{State (Observation)}
Dictionary with:
\begin{itemize}[leftmargin=*]
  \item \textbf{counts} (MultiDiscrete, size 3): total item counts.
  \item \textbf{my\_utilities} (MultiDiscrete, size 3): agent utilities \(u_A \in \{0..10\}^3\).
  \item \textbf{partner\_utilities} (MultiDiscrete, size 3): included if \texttt{reveal\_partner\_utilities=true}, else zeros.
  \item \textbf{last\_partner\_act} (Discrete(5)): last partner dialogue act.
  \item \textbf{last\_partner\_offer\_for\_me} (MultiDiscrete, size 3): last partner offer for agent.
  \item \textbf{turns\_remaining} (Discrete): remaining turns up to \texttt{max\_turns}.
\end{itemize}

\subsection{Actions}
\textbf{Dict}: \{\texttt{act\_type} (Discrete(5)), \texttt{oA} (MultiDiscrete, size 3)\}. Acts:
\begin{itemize}[leftmargin=*]
  \item 0: PROPOSE(\texttt{oA})\quad 1: INSIST(\texttt{oA})\quad 2: AGREE\quad 3: DISAGREE\quad 4: END
\end{itemize}
Action masking provided in \texttt{info[action\_mask]} (AGREE valid only if partner proposed/insisted). Per-dimension caps for \texttt{oA} are exposed via \texttt{info[oA\_max]}.

\subsection{Rewards and Termination}
\begin{itemize}[leftmargin=*]
  \item If both agree on a valid allocation with conservation \(o_A + o_B = i\): reward \(r_A = u_A \cdot o_A\), partner receives \(u_B \cdot o_B\).
  \item Episodes terminate on AGREE, END (final selection), or when \texttt{turns\_remaining} reaches zero.
\end{itemize}
A simple heuristic partner policy accepts offers meeting a utility ratio threshold (0.55 default; 0.5 if the agent used INSIST), else counter-proposes greedily.

\subsection{Context Sampling and Dataset}
By default, contexts are sampled uniformly over counts/utilities. If \texttt{use\_dataset=true}, contexts are loaded from \texttt{deal\_or\_no\_dialog.py} (\texttt{self\_play} or \texttt{dialogues}); optional normalization enforces a points budget.

\section{LLM Prompt and Hinting Mechanism}
\subsection{Prompt Template}
Path: \texttt{configs/llm\_prompt.txt}. The template enforces a strict single-line JSON output with keys \texttt{act\_type}, \texttt{oA}, and \texttt{confidence}, and embeds current observation/context fields. Example lines from the template:\newline
\emph{``Return ONLY one JSON object on a single line with keys: act\_type (0--4), oA ([3 ints]), confidence (0--1).''}

Template variables include: \texttt{counts\_csv}, \texttt{my\_utils\_csv}, \texttt{last\_partner\_act\_token}, \texttt{last\_offer\_csv}, \texttt{turns}, \texttt{p}, \texttt{history\_str}. Prompt rendering performed by \texttt{llm/prompt.py} with optional history length capping.

\subsection{Hint Injection Wrapper}
\textbf{Wrapper}: \texttt{env\_wrappers/hint\_injector.py} (\texttt{HintInjectorEnv}). Every \(k\) steps (and on reset), a hint is requested and converted to features:
\begin{itemize}[leftmargin=*]
  \item \textbf{act\_onehot} (5), \textbf{oA\_norm} (3, normalized by caps), \textbf{confidence} (1), \textbf{h\_avail} (1)
\end{itemize}
These are placed into \texttt{info[hint\_features]} and concatenated to base features by the \texttt{HintAdapter}. Providers:
\begin{itemize}[leftmargin=*]
  \item \textbf{random}: uniformly legal actions and capped \texttt{oA}.
  \item \textbf{expert}: uses a PPO or supervised expert checkpoint if available; greedy fallback otherwise.
  \item \textbf{llm}: uses GROQ or local HF client; strict JSON parsing and legality checks vs. \texttt{action\_mask}.
\end{itemize}
Retry/backoff with exponential delays; illegal or failed hints yield neutral features.

\section{RL Training Setup}
\subsection{Algorithms}
\textbf{PPO} and \textbf{REINFORCE} implementations consume concatenated base + hint features via a shared policy network with heads for act type, three \texttt{oA} categoricals, and value.

\subsection{Configs and Hyperparameters}
Default config files: \texttt{configs/ppo\_config.yaml}, \texttt{configs/reinforce\_config.yaml}. Key fields:
\begin{itemize}[leftmargin=*]
  \item \textbf{Env}: \texttt{id=DealOrNoDialog-v0}, \texttt{max\_turns=10}.
  \item \textbf{Hints}: \texttt{mode \in \{none, random, llm, expert\}}, cadence \(k\), prompt path, provider, and models.
  \item \textbf{Training (PPO)}: lr=3e-4, \(\gamma=0.99\), \(\lambda=0.95\), clip=0.2, epochs=4, minibatch=64, ent=0.0, vf=0.5, grad\_norm=0.5, rollout=128; steps via \texttt{num\_train\_steps}.
  \item \textbf{Training (REINFORCE)}: lr=3e-4, \(\gamma=0.99\), ent=0.01; steps via \texttt{num\_train\_steps}.
  \item \textbf{Logging}: output dir, CSV/TensorBoard, save cadence.
\end{itemize}

\subsection{Feature Wiring}
The \texttt{HintAdapter} builds base features from observation: counts (3), my utilities (3), optional partner utilities (3), last act one-hot (5), last partner offer (3), turns (1). Hint features add 10 dims. Action masking and \texttt{oA} caps are applied in both sampling and loss.

\subsection{Opponents and Curriculum}
The built-in partner is a heuristic (accepts above a ratio threshold, else greedy counter-offer). No staged curriculum is implemented by default; curricula can be emulated by varying \texttt{max\_turns}, dataset usage, hint mode/provider, or switching opponent to \texttt{expert}.

\subsection{Typical Runs}
\begin{itemize}[leftmargin=*]
  \item PPO example: \texttt{--algo ppo --hint llm --config configs/ppo\_config.yaml}
  \item REINFORCE example: \texttt{--algo reinforce --hint none --config configs/reinforce\_config.yaml}
\end{itemize}

\section{Prompt Examples}
Template includes inlined examples; e.g., when partner proposes the full allocation and it matches the agent's utilities, the model should output AGREE with high confidence. The system additionally adds a strict system message in remote calls to force a single-line JSON.

\section{References}
Main: 2303.00001v1 (attached). See also Lewis et al. (2017), Kwon et al. (2021).

\end{document}